{"cells":[{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"8eda830416514c198d2e176aa48f55f6","deepnote_cell_type":"text-cell-p"},"source":"","block_group":"8db2a08fd07c46fabdeb3050b5e711c0"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"350120359c3a418ea0b3d107e297c2c2","deepnote_cell_type":"text-cell-h1"},"source":"# Sentiment Analyzer\r\n","block_group":"b5450ced194c4244a41d82dbd5f7a25c"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"5ac41ec20c874bee8f2fc0dd63bce048","deepnote_cell_type":"text-cell-p"},"source":"This project utilizes NLP libraries to preprocess data, analyze it, and predict new opinions based on previous data (machine learning).\r\nI obtained a dataset of 150,000 samples from the website quera.org. Additionally, I acquired a larger dataset from kaggle.com and included it in the repository under the name big_train.","block_group":"a880157063894b088ebd27a5198a22c7"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"bc0e045402e7457987b191cb8ae6201a","deepnote_cell_type":"text-cell-p"},"source":"To train the model using the smaller dataset, use the following code:","block_group":"4d5482a7a86c45a2ae605cdacac6ea28"},{"cell_type":"code","metadata":{"source_hash":"d9d36e18","execution_start":1742857295402,"execution_millis":1,"execution_context_id":"fb64d43e-c606-4fed-a54f-587101cb25e2","cell_id":"5052dcc7d1b347bca698bbbaf688a053","deepnote_cell_type":"code"},"source":"train_data = pd.read_csv('train.csv')","block_group":"f7c3900bbaf7436e8f0932dd75e703ff","execution_count":1,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"0574018c35e94271b825e0fccc1d83c3","deepnote_cell_type":"text-cell-p"},"source":"If you have powerful hardware to process a large dataset, you can use the following command:","block_group":"1bff15168f744980815eb3374d275ddc"},{"cell_type":"code","metadata":{"source_hash":"319899d2","execution_start":1742857295462,"execution_millis":1,"execution_context_id":"fb64d43e-c606-4fed-a54f-587101cb25e2","cell_id":"4ed3eb97fc3a43f1b834ff12eaec59d4","deepnote_cell_type":"code"},"source":"train_data = pd.read_csv('big_train.csv', usecols=['body', 'recommendation_status'])","block_group":"be5f8d62fbd843e2aecc66cb09be174b","execution_count":2,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"a77ee142810d4e97b3f8ad23df1d75cf","deepnote_cell_type":"text-cell-p"},"source":"This dataset is a combination of the dataset from Quera and the dataset from Kaggle.","block_group":"81b3038478bf4f3582ee5c025068ed68"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"304daee3546e4e56aac755c0e7d3f9df","deepnote_cell_type":"text-cell-p"},"source":"I ran this project on hardware with the following specifications:\r\n2 vCPU, 5GB memory","block_group":"4f42029cabab46e59074f1aaa091e9a1"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"2b711e50d1aa4f14ac0c310d5e0f2386","deepnote_cell_type":"text-cell-p"},"source":"For the first dataset, I achieved an accuracy of approximately 60%, and for the larger dataset, I achieved an accuracy of approximately 75%.","block_group":"782f4a0ab8264dc5a6f95c8b3c8e4d8b"},{"cell_type":"markdown","metadata":{"cell_id":"4ca672f5185c4b5ab38eb1fbb0ad33eb","deepnote_cell_type":"separator"},"source":"<hr>","block_group":"d4dfb8f9e9594005b16b3912f76053e0"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"4279063ad17d4f3a81ac6809cb308a65","deepnote_cell_type":"text-cell-h2"},"source":"## Import Required Libraries\r","block_group":"235c86d1585d40a3833320653694c5a3"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"35643dc030f84776b62ee204fa9cab1b","deepnote_cell_type":"text-cell-p"},"source":"First, we import the necessary libraries. You can install the required libraries using the requirements.txt file provided in the repository.","block_group":"1dc5a81d9d784a73b8a981c04ba1f93c"},{"cell_type":"code","metadata":{"source_hash":"220a8844","execution_start":1742857295512,"execution_millis":1019,"execution_context_id":"fb64d43e-c606-4fed-a54f-587101cb25e2","cell_id":"8c7b35978428443da0d0b7aa04d9c336","deepnote_cell_type":"code"},"source":"import pandas as pd\nfrom hazm import Normalizer, word_tokenize, Stemmer, stopwords_list\nimport re\nfrom tqdm import tqdm\nfrom gensim.models import Word2Vec\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score","block_group":"216d6afd1fe2462295c9a2508c779b45","execution_count":3,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"cell_id":"cd4052435da14f39bf904b546efd1e05","deepnote_cell_type":"separator"},"source":"<hr>","block_group":"19c30ac976c047878fbe86b5ec67496f"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"74aecc930b7b41e6b79a112fe14281fb","deepnote_cell_type":"text-cell-h2"},"source":"## Load the Dataset\r","block_group":"012c1f7d03434ad8807eab61dff25b45"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"9efa77d9a0af4d21ab1ae24f92f56770","deepnote_cell_type":"text-cell-p"},"source":"In this step, we read the dataset files. The following code is based on the dataset from quera.org. Depending on your hardware capabilities, you can modify the code to load the larger dataset (big_train.csv).","block_group":"61cfa711f43a4abf9c358ee46ca5530b"},{"cell_type":"code","metadata":{"source_hash":"f3bb704d","execution_start":1742857296592,"execution_millis":422,"execution_context_id":"fb64d43e-c606-4fed-a54f-587101cb25e2","cell_id":"8b8504ac39b946c4936bee9a1df75031","deepnote_cell_type":"code"},"source":"train_data = pd.read_csv('train.csv') \ntest_data = pd.read_csv('test.csv')","block_group":"5c4e5c75de774c76b7c28fa87f51c5b0","execution_count":4,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"22e68773993940968dac99d87d1947ff","deepnote_cell_type":"text-cell-p"},"source":"After loading the dataset, we can use the following commands to gather information and insights about the data:","block_group":"2960a43b1037465c87b0cf79d8c5cecb"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"4eae793242f84cf2a6bea5e24c0aa425","deepnote_cell_type":"text-cell-p"},"source":"","block_group":"133f13b41eae4cf09e0af4eab4c22357"},{"cell_type":"code","metadata":{"source_hash":"9456a8b0","execution_start":1742857297092,"execution_millis":1,"execution_context_id":"fb64d43e-c606-4fed-a54f-587101cb25e2","cell_id":"cd09e0c997c04b5c8d98ba9bf6421985","deepnote_cell_type":"code"},"source":"train_data.info()","block_group":"748a7fce28c246bba2d28af2b7ea2f44","execution_count":5,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 149400 entries, 0 to 149399\nData columns (total 2 columns):\n #   Column                 Non-Null Count   Dtype \n---  ------                 --------------   ----- \n 0   body                   149400 non-null  object\n 1   recommendation_status  149400 non-null  object\ndtypes: object(2)\nmemory usage: 2.3+ MB\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/32318887-f544-494a-b5ab-0ea617612539","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"61e84650","execution_start":1742857297162,"execution_millis":1,"execution_context_id":"fb64d43e-c606-4fed-a54f-587101cb25e2","cell_id":"d849d08bfb504554bb660ff9982eb1a2","deepnote_cell_type":"code"},"source":"test_data.info()","block_group":"06a649590fcc46858a21f0e1698c5695","execution_count":6,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 600 entries, 0 to 599\nData columns (total 1 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   body    600 non-null    object\ndtypes: object(1)\nmemory usage: 4.8+ KB\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/a20bf6a6-a3c0-4049-b37f-873c484bd580","content_dependencies":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"24def52c3b3c4825b4735746f123b679","deepnote_cell_type":"text-cell-p"},"source":"Using the last command, you can view the count of recommended, not_recommended, and no_idea reviews in the dataset:","block_group":"9d2c1132bd88460889d8bc0f73a222cb"},{"cell_type":"code","metadata":{"source_hash":"e7060ff8","execution_start":1742857297222,"execution_millis":0,"execution_context_id":"fb64d43e-c606-4fed-a54f-587101cb25e2","cell_id":"8b4213e50b164649b91e80cabb03aee3","deepnote_cell_type":"code"},"source":"train_data['recommendation_status'].value_counts()","block_group":"19583f7a394b415c8b73d0a972921c60","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"recommendation_status\nnot_recommended    49800\nrecommended        49800\nno_idea            49800\nName: count, dtype: int64"},"metadata":{}}],"outputs_reference":"dbtable:cell_outputs/c7fde862-cb6f-46cc-9397-b431ce959084","content_dependencies":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"b1ea13616ee740dcae0df23924c6bce4","deepnote_cell_type":"text-cell-p"},"source":"","block_group":"c254f11086c54331b64c06eb77e0e9e0"},{"cell_type":"markdown","metadata":{"cell_id":"4d455aa144f845a9882d6195f66ceac0","deepnote_cell_type":"separator"},"source":"<hr>","block_group":"38881ee0ffa348169c608c3ea2740bce"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"a47e62c601eb47528da57a005cdcd7e3","deepnote_cell_type":"text-cell-h2"},"source":"## Handle Missing Values and Encode Labels\r","block_group":"427d91d6f5a24f7aa16576d41e2ab59b"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"9117c99fcfa141fca0a80fb7a3d3422b","deepnote_cell_type":"text-cell-p"},"source":"In this step, we will:","block_group":"47aa951651d849bbb29d321c68cf4e99"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"59acf17b5150494082e52c741683a71f","deepnote_cell_type":"text-cell-p"},"source":"Fill null and NaN values in the dataset with appropriate values (e.g., no_idea).","block_group":"60dacf868e5341a4bf16a95349b16634"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"bf0b25ec36a146c2988aa46eaca2239b","deepnote_cell_type":"text-cell-p"},"source":"Map the recommendation status labels to numerical values for easier processing.","block_group":"630fa9b72972480282ad635d237a045c"},{"cell_type":"code","metadata":{"source_hash":"6857d285","execution_start":1742857297272,"execution_millis":0,"execution_context_id":"fb64d43e-c606-4fed-a54f-587101cb25e2","cell_id":"f90483b36c54488083301df1326c9a59","deepnote_cell_type":"code"},"source":"# Replace Nan and Null with no_idea Lable\n# Convert \"recommended\" data to 1 and \"not_recommended\" data to 0.\n\ntrain_data[\"recommendation_status\"] = train_data[\"recommendation_status\"].fillna(\"no_idea\")\n\nvalid_statuses = {\"no_idea\", \"recommended\", \"not_recommended\"}\ntrain_data[\"recommendation_status\"] = train_data[\"recommendation_status\"].apply(\n    lambda x: x if x in valid_statuses else \"no_idea\"\n)\n\ntrain_data[\"recommendation_status\"] = train_data[\"recommendation_status\"].map({\n    \"no_idea\": 2,\n    \"recommended\": 1,\n    \"not_recommended\": 0\n})","block_group":"2cf5796032c44a11b51f1b86bf764af2","execution_count":8,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"5391fd16a11f430d8b52b2452dc8b20a","deepnote_cell_type":"text-cell-p"},"source":"Now, we need to verify whether the preprocessing steps (handling null values and encoding labels) have been performed correctly. We can do this by checking the dataset for any remaining issues and confirming the changes.","block_group":"d08ab1281e67483b86b5a1d1ecbd8c79"},{"cell_type":"code","metadata":{"source_hash":"88b869a4","execution_start":1742857297322,"execution_millis":0,"execution_context_id":"fb64d43e-c606-4fed-a54f-587101cb25e2","cell_id":"4e43957d0208487e8abfa5a850bcb124","deepnote_cell_type":"code"},"source":"# checking the values stored in \"recommendation_starus\"\ntrain_data[\"recommendation_status\"].unique()","block_group":"502dee968caa4e2188d145614ef7136e","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"array([0, 1, 2])"},"metadata":{}}],"outputs_reference":"dbtable:cell_outputs/687a6a3b-add6-4bd3-bd34-719a33bbaab0","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"ea2d4a61","execution_start":1742857297382,"execution_millis":0,"execution_context_id":"fb64d43e-c606-4fed-a54f-587101cb25e2","cell_id":"f877289990384e6abf4a58128b3a85ba","deepnote_cell_type":"code"},"source":"train_data[\"recommendation_status\"].value_counts()","block_group":"d3f86aa7c868426f8d6be115a77cb47d","execution_count":10,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"recommendation_status\n0    49800\n1    49800\n2    49800\nName: count, dtype: int64"},"metadata":{}}],"outputs_reference":"dbtable:cell_outputs/42537004-17cb-4c94-a454-6ad00f5509ea","content_dependencies":null},{"cell_type":"markdown","metadata":{"cell_id":"ebfd834445f54ec8b35d6001a56c8e86","deepnote_cell_type":"separator"},"source":"<hr>","block_group":"b1ab8a2fa49f4f99b87bc1f8d5b34d85"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"94a5e91eba2d4b5da1bb58ad4d3eb022","deepnote_cell_type":"text-cell-h2"},"source":"## Define a Text Preprocessing Function\r","block_group":"7fede5ab1dac447b92add55544df224b"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"c65025e3d4b64dc6880b9026a59ad314","deepnote_cell_type":"text-cell-p"},"source":"In this step, we will write a function to preprocess the text data. This function will perform the following tasks:","block_group":"655f0d7bd52b46d5a845e5c5590e1ab9"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"fcd52822f6da4497b988628af7726ac1","deepnote_cell_type":"text-cell-p"},"source":"Normalize text: Convert text to lowercase.","block_group":"98d858fe18e740afb402fa27ddb353f2"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"c2eea27dd49d4b55aee7e65ff0c88ff4","deepnote_cell_type":"text-cell-p"},"source":"Tokenize: Split text into individual words or tokens.","block_group":"d293a015d8294519a5fbcbf3fa24eff9"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"efddc8a0da2345eabba3e186402f7f0a","deepnote_cell_type":"text-cell-p"},"source":"Remove stopwords: Eliminate common words that do not contribute much to the meaning (e.g., \"و\", \"که\", \"چون\").","block_group":"cf577f27c5c743ce9e4482486f396d6e"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"6bc4ad8f29bf415c80603e146bb40996","deepnote_cell_type":"text-cell-p"},"source":"Stemming/Lemmatization: Reduce words to their root form (e.g., \"کتاب ها\" -> \"کتاب\" ).","block_group":"cbcc30c3975e49ab909fac944bfce16a"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"b5ea3533d80b466593e5bcfabaf8df9d","deepnote_cell_type":"text-cell-p"},"source":"Remove special characters and numbers: Clean the text by removing unnecessary symbols and digits.","block_group":"56882ef091324c60a71c7cff161a5fa2"},{"cell_type":"code","metadata":{"source_hash":"8703dfa2","execution_start":1742857297442,"execution_millis":1403,"execution_context_id":"fb64d43e-c606-4fed-a54f-587101cb25e2","cell_id":"3c2dd157319f4b65bd832b46b344ad61","deepnote_cell_type":"code"},"source":"# Initialize tools\nstopwords = set(stopwords_list())  # Convert to set for faster lookup\nnormalizer = Normalizer()\nstemmer = Stemmer()\n\n# Define regex patterns\npunctuations = r'[!()-\\[\\]{};:\\'\",؟<>./?@#$%^&*_~]'\nnumbers_regex = r'[۰-۹\\d]+'  # Combined Persian and Latin numbers\nwhite_space = r'\\s+'\n\ndef preprocess_text(text):\n    # Normalize text\n    text = normalizer.normalize(str(text))\n    \n    # Remove numbers and punctuations\n    text = re.sub(numbers_regex, '', text)  # Remove all numbers\n    text = re.sub(punctuations, ' ', text)  # Replace punctuations with space\n    \n    # Normalize whitespace\n    text = re.sub(white_space, ' ', text).strip()  # Replace multiple spaces with single space\n    \n    # Tokenize and process tokens\n    tokens = word_tokenize(text)\n    processed_tokens = [\n        stemmer.stem(token)  # Stem each token\n        for token in tokens\n        if token not in stopwords and token.strip()  # Remove stopwords and empty tokens\n    ]\n    \n    return processed_tokens","block_group":"0a6e03879b0746d58b14c0339d2607ee","execution_count":11,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"a2a539754d80436d97b17bd983946406","deepnote_cell_type":"text-cell-p"},"source":"We will now test the preprocess_text function on a sample input to ensure it works as expected. The expected output are as follows:","block_group":"89fb7cadd8d34a1ca79299829f877f0e"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"fa37ff43c1dd4922851484b4d339feb2","deepnote_cell_type":"text-cell-p"},"source":"['متولد', 'سال', 'هس']","block_group":"4d915a1f5bce40ea8fa094c17e47f527"},{"cell_type":"code","metadata":{"source_hash":"746b172d","execution_start":1742857298893,"execution_millis":531,"execution_context_id":"fb64d43e-c606-4fed-a54f-587101cb25e2","cell_id":"e5bc25b570ab4082aff3da8eed82dd15","deepnote_cell_type":"code"},"source":"exmpale = \"من متولد سال ۱۳۷۷ هستم\"\npreprocess_text(exmpale)","block_group":"044c286f147241a1abc90c3135453498","execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"['متولد', 'سال', 'هس']"},"metadata":{}}],"outputs_reference":"dbtable:cell_outputs/fb4741a1-82c3-4421-a078-f375686a060f","content_dependencies":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"f54a1bb6f05b45c0b3b5ec4c0e815254","deepnote_cell_type":"text-cell-p"},"source":"Now that the preprocessing function is ready, we will apply it to all the reviews in the train_data dataset. This will prepare the data for use with the Word2Vec model. We will store the preprocessed data in a new column called preprocess.","block_group":"09440e6453f84ee886e44599a0f51d01"},{"cell_type":"code","metadata":{"source_hash":"95bdda57","execution_start":1742857299483,"execution_millis":37501,"execution_context_id":"fb64d43e-c606-4fed-a54f-587101cb25e2","cell_id":"942ff99460e4471083933caef96917ff","deepnote_cell_type":"code"},"source":"dataes = train_data['body']\n\ndef process_chunks(series, chunk_size=1000):\n    chunks = [series[i:i + chunk_size] for i in range(0, len(series), chunk_size)]\n    processed_data = []\n    \n    for chunk in tqdm(chunks, desc=\"Processing chunks\"):\n        processed_chunk = chunk.apply(preprocess_text)\n        processed_data.extend(processed_chunk)\n    \n    return pd.Series(processed_data)\n\n# Process data in chunks with progress bar\ndata_processed = process_chunks(dataes)","block_group":"20082ddc1bfe4fa3b724bc57ebfdad19","execution_count":13,"outputs":[{"name":"stderr","text":"Processing chunks: 100%|██████████| 150/150 [00:37<00:00,  4.00it/s]\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/7220596a-be77-4db8-a3e2-30962bcc7264","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"d0646854","execution_start":1742857337042,"execution_millis":1,"execution_context_id":"fb64d43e-c606-4fed-a54f-587101cb25e2","cell_id":"283c755321eb4c938e296d19a2560d87","deepnote_cell_type":"code"},"source":"train_data[\"preprocess\"] = data_processed\ntrain_data.head()","block_group":"094d4b5df05d4b6b83af8fecfcb70058","execution_count":14,"outputs":[{"output_type":"execute_result","execution_count":14,"data":{"application/vnd.deepnote.dataframe.v3+json":{"column_count":3,"row_count":5,"columns":[{"name":"body","dtype":"object","stats":{"unique_count":5,"nan_count":0,"categories":[{"name":"جنسش‌خوب‌بود‌خیلی‌بدبدبود","count":1},{"name":"به کار میاد شک ندارم","count":1},{"name":"3 others","count":3}]}},{"name":"recommendation_status","dtype":"int64","stats":{"unique_count":3,"nan_count":0,"min":"0","max":"2","histogram":[{"bin_start":0,"bin_end":0.2,"count":1},{"bin_start":0.2,"bin_end":0.4,"count":0},{"bin_start":0.4,"bin_end":0.6000000000000001,"count":0},{"bin_start":0.6000000000000001,"bin_end":0.8,"count":0},{"bin_start":0.8,"bin_end":1,"count":0},{"bin_start":1,"bin_end":1.2000000000000002,"count":1},{"bin_start":1.2000000000000002,"bin_end":1.4000000000000001,"count":0},{"bin_start":1.4000000000000001,"bin_end":1.6,"count":0},{"bin_start":1.6,"bin_end":1.8,"count":0},{"bin_start":1.8,"bin_end":2,"count":3}]}},{"name":"preprocess","dtype":"object","stats":{"unique_count":5,"nan_count":0,"categories":[{"name":"['جنسش\\u200cخوب\\u200cبود\\u200cخیلی\\u200cبدبدبود']","count":1},{"name":"['کار', 'میاد', 'شک', 'ندار']","count":1},{"name":"3 others","count":3}]}},{"name":"_deepnote_index_column","dtype":"int64"}],"rows":[{"body":"جنسش‌خوب‌بود‌خیلی‌بدبدبود","recommendation_status":0,"preprocess":"['جنسش\\u200cخوب\\u200cبود\\u200cخیلی\\u200cبدبدبود']","_deepnote_index_column":0},{"body":"به کار میاد شک ندارم","recommendation_status":1,"preprocess":"['کار', 'میاد', 'شک', 'ندار']","_deepnote_index_column":1},{"body":"چیزی ک توعکسه واست میفرستن ولی هم جنسش خوب نیست هم خیلی کوچیکتره صفهش انگار زدی زیر گوشه ی بچه ساعتشو ازش گرفتی آنقدر کوچیکه","recommendation_status":2,"preprocess":"['ک', 'توعکسه', 'واس', 'میفرستن', 'جنس', 'کوچیکتره', 'صفه', 'انگار', 'زد', 'گوشه', 'بچه', 'ساعتشو', 'از', 'گرفت', 'آنقدر', 'کوچیکه']","_deepnote_index_column":2},{"body":"رنگش خیلی خوبه . براق هم هست و زود خشک میشه . من سه دور زدم توی عکس \r\nفقط فرچه اش کوچیک بود و جا مینداخت اما در کل راضی بودم","recommendation_status":2,"preprocess":"['رنگ', 'خوبه', 'براق', 'هس', 'زود', 'خشک', 'میشه', 'زد', 'تو', 'عکس', 'فرچه', 'کوچیک', 'مینداخ', 'راض', 'بود']","_deepnote_index_column":3},{"body":"من مرجوع کردم قسمت پاچه شلوار برام تنگ بود ولی جنسش بد به نظر نمیومد","recommendation_status":2,"preprocess":"['مرجوع', 'قسم', 'پاچه', 'شلوار', 'برا', 'تنگ', 'جنس', 'بد', 'نمیومد']","_deepnote_index_column":4}],"type":"dataframe"},"text/plain":"                                                body  recommendation_status  \\\n0                          جنسش‌خوب‌بود‌خیلی‌بدبدبود                      0   \n1                               به کار میاد شک ندارم                      1   \n2  چیزی ک توعکسه واست میفرستن ولی هم جنسش خوب نیس...                      2   \n3  رنگش خیلی خوبه . براق هم هست و زود خشک میشه . ...                      2   \n4  من مرجوع کردم قسمت پاچه شلوار برام تنگ بود ولی...                      2   \n\n                                          preprocess  \n0                        [جنسش‌خوب‌بود‌خیلی‌بدبدبود]  \n1                              [کار, میاد, شک, ندار]  \n2  [ک, توعکسه, واس, میفرستن, جنس, کوچیکتره, صفه, ...  \n3  [رنگ, خوبه, براق, هس, زود, خشک, میشه, زد, تو, ...  \n4  [مرجوع, قسم, پاچه, شلوار, برا, تنگ, جنس, بد, ن...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>body</th>\n      <th>recommendation_status</th>\n      <th>preprocess</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>جنسش‌خوب‌بود‌خیلی‌بدبدبود</td>\n      <td>0</td>\n      <td>[جنسش‌خوب‌بود‌خیلی‌بدبدبود]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>به کار میاد شک ندارم</td>\n      <td>1</td>\n      <td>[کار, میاد, شک, ندار]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>چیزی ک توعکسه واست میفرستن ولی هم جنسش خوب نیس...</td>\n      <td>2</td>\n      <td>[ک, توعکسه, واس, میفرستن, جنس, کوچیکتره, صفه, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>رنگش خیلی خوبه . براق هم هست و زود خشک میشه . ...</td>\n      <td>2</td>\n      <td>[رنگ, خوبه, براق, هس, زود, خشک, میشه, زد, تو, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>من مرجوع کردم قسمت پاچه شلوار برام تنگ بود ولی...</td>\n      <td>2</td>\n      <td>[مرجوع, قسم, پاچه, شلوار, برا, تنگ, جنس, بد, ن...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"outputs_reference":"s3:deepnote-cell-outputs-production/b4f87157-fdfc-4fe8-80a2-7a8f3ac11efe","content_dependencies":null},{"cell_type":"markdown","metadata":{"cell_id":"3d7d85356e87483ba571891c056c91e8","deepnote_cell_type":"separator"},"source":"<hr>","block_group":"ed90e89c366a42a38dca995167ef4468"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"cb0317dfc4314b0fba14dbf09dc8ea48","deepnote_cell_type":"text-cell-h2"},"source":"## Embedding Data Using Word2Vec\r","block_group":"0c5254a1fb4e4429b3d2c5d4ab78f7ca"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"adccfc88c6bb40b787bd48f219c4604f","deepnote_cell_type":"text-cell-p"},"source":"Now that the data has been preprocessed and stored in the preprocess column, we will use the Word2Vec algorithm to convert words into numerical vectors. This step involves training a Word2Vec model on the preprocessed text data to create word embeddings.","block_group":"c16f07a384c64d0586c8a771987c1739"},{"cell_type":"code","metadata":{"source_hash":"73330793","execution_start":1742857337102,"execution_millis":5202,"execution_context_id":"fb64d43e-c606-4fed-a54f-587101cb25e2","cell_id":"d5f3aab803464c789ec0c3af1077d7b2","deepnote_cell_type":"code"},"source":"model = Word2Vec(sentences=train_data[\"preprocess\"], vector_size=100, window=5, min_count=1, workers=4)","block_group":"cedb2f98c0814dd29e15984a5574df72","execution_count":15,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"0d6c76cf17904ac9a4f451294f210980","deepnote_cell_type":"text-cell-p"},"source":"Now that the Word2Vec model has been trained, we will test it by finding words that are most similar to the word \"دوست\" (friend). This will help us evaluate the quality of the embeddings and understand how well the model has captured semantic relationships.","block_group":"95ed5d5e65cd446f968ac913ab60af51"},{"cell_type":"code","metadata":{"source_hash":"73ce976b","execution_start":1742857342363,"execution_millis":0,"execution_context_id":"fb64d43e-c606-4fed-a54f-587101cb25e2","cell_id":"d3b2232042c3481fa742fd47a5a4de72","deepnote_cell_type":"code"},"source":"model.wv.most_similar(\"دوست\")","block_group":"0526bff2465d42409f76241b33de1268","execution_count":16,"outputs":[{"output_type":"execute_result","execution_count":16,"data":{"text/plain":"[('دوسشون', 0.934141993522644),\n ('دوستشون', 0.879103422164917),\n ('دوس', 0.8512587547302246),\n ('اصرار', 0.7907397747039795),\n ('انتظارشو', 0.7793211936950684),\n ('درضدک', 0.75844407081604),\n ('عاشقشه', 0.743995189666748),\n ('وارم', 0.7315830588340759),\n ('hq', 0.7278368473052979),\n ('تیوپیشو', 0.7266822457313538)]"},"metadata":{}}],"outputs_reference":"dbtable:cell_outputs/a8fa0b2f-a1ee-4264-a282-13b31b4b8583","content_dependencies":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"2c751b94595a4e48befefdfde742ef0c","deepnote_cell_type":"text-cell-p"},"source":"In this step, we will design a function called sentence_vector that calculates the embedding vector for each review by averaging the word vectors of all the words in the review. This will produce a single, fixed-size vector for each sentence, which can be used as input for machine learning models.","block_group":"4a93dd5a6fcc43f18af6f10961d75c83"},{"cell_type":"code","metadata":{"source_hash":"72b3ff03","execution_start":1742857342422,"execution_millis":0,"execution_context_id":"fb64d43e-c606-4fed-a54f-587101cb25e2","cell_id":"e41ca930e50a4cb295b1fe2fa27393c3","deepnote_cell_type":"code"},"source":"# Create sentence vectors by averaging word vectors\ndef sentence_vector(sentence):\n    vectors = []\n    for word in sentence:\n        try:\n            vectors.append(model.wv[word])\n        except KeyError:\n            # Handle words not in vocabulary (e.g., use a zero vector)\n            vectors.append(np.zeros(100))  # Assuming vector_size=100\n    if vectors:\n        return np.mean(vectors, axis=0)\n    else:\n        return np.zeros(100)","block_group":"4c87bff9dfba4feabf1b5b7e3fc0e215","execution_count":17,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"c3ce74abf3634806a855c2103f900db0","deepnote_cell_type":"text-cell-p"},"source":"Now that the sentence_vector function is defined, we will apply it to the preprocess column of the train_data dataset. This will convert each review into its corresponding sentence vector. The results will be stored in a variable called sentence_vectors.","block_group":"713475a4723646bda7ded6534d140334"},{"cell_type":"code","metadata":{"source_hash":"80bea63e","execution_start":1742857342482,"execution_millis":4343,"execution_context_id":"fb64d43e-c606-4fed-a54f-587101cb25e2","cell_id":"91b7465a659546d99f7bbb3cc0a07630","deepnote_cell_type":"code"},"source":"sentence_vectors = train_data['preprocess'].apply(sentence_vector)\nsentence_vectors","block_group":"b70083e5be12421aa144268fa237f3cc","execution_count":18,"outputs":[{"output_type":"execute_result","execution_count":18,"data":{"text/plain":"0         [0.008226887, -0.006862732, 0.009667573, -0.00...\n1         [-0.31554842, -0.22631446, -0.16765557, 0.8496...\n2         [0.0068331547, -0.41255942, 0.036244158, 0.139...\n3         [-0.22793739, -0.083498545, -0.26247945, 0.424...\n4         [-0.04690854, -0.6549899, -0.10525497, -0.3781...\n                                ...                        \n149395    [-0.32803124, -0.12798204, 0.3178891, 0.275178...\n149396    [-0.19986522, 0.22526328, 0.1352489, 0.0008057...\n149397    [0.71512026, -0.9660871, -0.37741694, 0.541157...\n149398    [-0.124842875, 0.1650039, -0.032411553, -0.180...\n149399    [-0.085027635, -0.9697184, -0.28658074, -0.163...\nName: preprocess, Length: 149400, dtype: object"},"metadata":{}}],"outputs_reference":"dbtable:cell_outputs/a401baf1-9bb8-4441-b3ca-ab918e2c90cd","content_dependencies":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"ce18ef53954840dcb5a233cee019b1c7","deepnote_cell_type":"text-cell-p"},"source":"In this step, we will split the dataset into training and evaluation sets using the train_test_split function. The data will be divided such that 80% is used for training and 20% is used for evaluation. Here, X represents the sentence vectors (embedding vectors for each review), and y represents the target labels (recommendation_status).","block_group":"f8a902fc742242fcae7deabe4f5344a5"},{"cell_type":"code","metadata":{"source_hash":"4b3aaf0c","execution_start":1742857346908,"execution_millis":66,"execution_context_id":"fb64d43e-c606-4fed-a54f-587101cb25e2","cell_id":"dc0c59610aa54096a87355758a36cf97","deepnote_cell_type":"code"},"source":"# Convert sentence vectors to a NumPy array\nX = np.array(sentence_vectors.to_list())\n\n# Assuming 'df[\"recommendation_status\"]' contains target labels\ny = train_data[\"recommendation_status\"].values\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","block_group":"6b579eaf44164eca8a6ad6be757386c4","execution_count":19,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"1a30f1d4474845a69a12887223264635","deepnote_cell_type":"text-cell-p"},"source":"After preparing the data and splitting it into training and evaluation sets, it’s time to train the model. In this project, we will use Logistic Regression for sentiment classification. The model will be trained using the fit method on the training data (X_train and y_train).","block_group":"8790216dd9be45ed89599fa9809bfca0"},{"cell_type":"code","metadata":{"source_hash":"be23fbaf","execution_start":1742857347022,"execution_millis":23964,"execution_context_id":"fb64d43e-c606-4fed-a54f-587101cb25e2","cell_id":"1a95e60784eb44d7991e42bd3d31af9c","deepnote_cell_type":"code"},"source":"# Initialize and train the Logistic Regression model\nlogistic_model = LogisticRegression(max_iter=1000)\nlogistic_model.fit(X_train, y_train)","block_group":"d265273cac9748169f3f61a73c357cd2","execution_count":20,"outputs":[{"output_type":"execute_result","execution_count":20,"data":{"text/plain":"LogisticRegression(max_iter=1000)","text/html":"<style>#sk-container-id-1 {\n  /* Definition of color scheme common for light and dark mode */\n  --sklearn-color-text: #000;\n  --sklearn-color-text-muted: #666;\n  --sklearn-color-line: gray;\n  /* Definition of color scheme for unfitted estimators */\n  --sklearn-color-unfitted-level-0: #fff5e6;\n  --sklearn-color-unfitted-level-1: #f6e4d2;\n  --sklearn-color-unfitted-level-2: #ffe0b3;\n  --sklearn-color-unfitted-level-3: chocolate;\n  /* Definition of color scheme for fitted estimators */\n  --sklearn-color-fitted-level-0: #f0f8ff;\n  --sklearn-color-fitted-level-1: #d4ebff;\n  --sklearn-color-fitted-level-2: #b3dbfd;\n  --sklearn-color-fitted-level-3: cornflowerblue;\n\n  /* Specific color for light theme */\n  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-icon: #696969;\n\n  @media (prefers-color-scheme: dark) {\n    /* Redefinition of color scheme for dark theme */\n    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-icon: #878787;\n  }\n}\n\n#sk-container-id-1 {\n  color: var(--sklearn-color-text);\n}\n\n#sk-container-id-1 pre {\n  padding: 0;\n}\n\n#sk-container-id-1 input.sk-hidden--visually {\n  border: 0;\n  clip: rect(1px 1px 1px 1px);\n  clip: rect(1px, 1px, 1px, 1px);\n  height: 1px;\n  margin: -1px;\n  overflow: hidden;\n  padding: 0;\n  position: absolute;\n  width: 1px;\n}\n\n#sk-container-id-1 div.sk-dashed-wrapped {\n  border: 1px dashed var(--sklearn-color-line);\n  margin: 0 0.4em 0.5em 0.4em;\n  box-sizing: border-box;\n  padding-bottom: 0.4em;\n  background-color: var(--sklearn-color-background);\n}\n\n#sk-container-id-1 div.sk-container {\n  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n     but bootstrap.min.css set `[hidden] { display: none !important; }`\n     so we also need the `!important` here to be able to override the\n     default hidden behavior on the sphinx rendered scikit-learn.org.\n     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n  display: inline-block !important;\n  position: relative;\n}\n\n#sk-container-id-1 div.sk-text-repr-fallback {\n  display: none;\n}\n\ndiv.sk-parallel-item,\ndiv.sk-serial,\ndiv.sk-item {\n  /* draw centered vertical line to link estimators */\n  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n  background-size: 2px 100%;\n  background-repeat: no-repeat;\n  background-position: center center;\n}\n\n/* Parallel-specific style estimator block */\n\n#sk-container-id-1 div.sk-parallel-item::after {\n  content: \"\";\n  width: 100%;\n  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n  flex-grow: 1;\n}\n\n#sk-container-id-1 div.sk-parallel {\n  display: flex;\n  align-items: stretch;\n  justify-content: center;\n  background-color: var(--sklearn-color-background);\n  position: relative;\n}\n\n#sk-container-id-1 div.sk-parallel-item {\n  display: flex;\n  flex-direction: column;\n}\n\n#sk-container-id-1 div.sk-parallel-item:first-child::after {\n  align-self: flex-end;\n  width: 50%;\n}\n\n#sk-container-id-1 div.sk-parallel-item:last-child::after {\n  align-self: flex-start;\n  width: 50%;\n}\n\n#sk-container-id-1 div.sk-parallel-item:only-child::after {\n  width: 0;\n}\n\n/* Serial-specific style estimator block */\n\n#sk-container-id-1 div.sk-serial {\n  display: flex;\n  flex-direction: column;\n  align-items: center;\n  background-color: var(--sklearn-color-background);\n  padding-right: 1em;\n  padding-left: 1em;\n}\n\n\n/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\nclickable and can be expanded/collapsed.\n- Pipeline and ColumnTransformer use this feature and define the default style\n- Estimators will overwrite some part of the style using the `sk-estimator` class\n*/\n\n/* Pipeline and ColumnTransformer style (default) */\n\n#sk-container-id-1 div.sk-toggleable {\n  /* Default theme specific background. It is overwritten whether we have a\n  specific estimator or a Pipeline/ColumnTransformer */\n  background-color: var(--sklearn-color-background);\n}\n\n/* Toggleable label */\n#sk-container-id-1 label.sk-toggleable__label {\n  cursor: pointer;\n  display: flex;\n  width: 100%;\n  margin-bottom: 0;\n  padding: 0.5em;\n  box-sizing: border-box;\n  text-align: center;\n  align-items: start;\n  justify-content: space-between;\n  gap: 0.5em;\n}\n\n#sk-container-id-1 label.sk-toggleable__label .caption {\n  font-size: 0.6rem;\n  font-weight: lighter;\n  color: var(--sklearn-color-text-muted);\n}\n\n#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n  /* Arrow on the left of the label */\n  content: \"▸\";\n  float: left;\n  margin-right: 0.25em;\n  color: var(--sklearn-color-icon);\n}\n\n#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n  color: var(--sklearn-color-text);\n}\n\n/* Toggleable content - dropdown */\n\n#sk-container-id-1 div.sk-toggleable__content {\n  max-height: 0;\n  max-width: 0;\n  overflow: hidden;\n  text-align: left;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content pre {\n  margin: 0.2em;\n  border-radius: 0.25em;\n  color: var(--sklearn-color-text);\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n  /* unfitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n  /* Expand drop-down */\n  max-height: 200px;\n  max-width: 100%;\n  overflow: auto;\n}\n\n#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n  content: \"▾\";\n}\n\n/* Pipeline/ColumnTransformer-specific style */\n\n#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator-specific style */\n\n/* Colorize estimator box */\n#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n#sk-container-id-1 div.sk-label label {\n  /* The background is the default theme color */\n  color: var(--sklearn-color-text-on-default-background);\n}\n\n/* On hover, darken the color of the background */\n#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n/* Label box, darken color on hover, fitted */\n#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator label */\n\n#sk-container-id-1 div.sk-label label {\n  font-family: monospace;\n  font-weight: bold;\n  display: inline-block;\n  line-height: 1.2em;\n}\n\n#sk-container-id-1 div.sk-label-container {\n  text-align: center;\n}\n\n/* Estimator-specific */\n#sk-container-id-1 div.sk-estimator {\n  font-family: monospace;\n  border: 1px dotted var(--sklearn-color-border-box);\n  border-radius: 0.25em;\n  box-sizing: border-box;\n  margin-bottom: 0.5em;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n/* on hover */\n#sk-container-id-1 div.sk-estimator:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Specification for estimator info (e.g. \"i\" and \"?\") */\n\n/* Common style for \"i\" and \"?\" */\n\n.sk-estimator-doc-link,\na:link.sk-estimator-doc-link,\na:visited.sk-estimator-doc-link {\n  float: right;\n  font-size: smaller;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1em;\n  height: 1em;\n  width: 1em;\n  text-decoration: none !important;\n  margin-left: 0.5em;\n  text-align: center;\n  /* unfitted */\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n  color: var(--sklearn-color-unfitted-level-1);\n}\n\n.sk-estimator-doc-link.fitted,\na:link.sk-estimator-doc-link.fitted,\na:visited.sk-estimator-doc-link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\ndiv.sk-estimator:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\ndiv.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n/* Span, style for the box shown on hovering the info icon */\n.sk-estimator-doc-link span {\n  display: none;\n  z-index: 9999;\n  position: relative;\n  font-weight: normal;\n  right: .2ex;\n  padding: .5ex;\n  margin: .5ex;\n  width: min-content;\n  min-width: 20ex;\n  max-width: 50ex;\n  color: var(--sklearn-color-text);\n  box-shadow: 2pt 2pt 4pt #999;\n  /* unfitted */\n  background: var(--sklearn-color-unfitted-level-0);\n  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n}\n\n.sk-estimator-doc-link.fitted span {\n  /* fitted */\n  background: var(--sklearn-color-fitted-level-0);\n  border: var(--sklearn-color-fitted-level-3);\n}\n\n.sk-estimator-doc-link:hover span {\n  display: block;\n}\n\n/* \"?\"-specific style due to the `<a>` HTML tag */\n\n#sk-container-id-1 a.estimator_doc_link {\n  float: right;\n  font-size: 1rem;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1rem;\n  height: 1rem;\n  width: 1rem;\n  text-decoration: none;\n  /* unfitted */\n  color: var(--sklearn-color-unfitted-level-1);\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n}\n\n#sk-container-id-1 a.estimator_doc_link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\n#sk-container-id-1 a.estimator_doc_link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n}\n</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LogisticRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(max_iter=1000)</pre></div> </div></div></div></div>"},"metadata":{}}],"outputs_reference":"s3:deepnote-cell-outputs-production/42ecaae9-872d-4c1b-911c-ab625dccc6c6","content_dependencies":null},{"cell_type":"markdown","metadata":{"cell_id":"9e533255d30b4ffd93c7780155b25dab","deepnote_cell_type":"separator"},"source":"<hr>","block_group":"c950d87e4d174be1a2d8fc22ca442f38"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"f7dfb9ab339e48388c20a6ba9e1b49aa","deepnote_cell_type":"text-cell-h2"},"source":"## Evaluate the Model\r","block_group":"09d648f6e86a44edb6b01e72e4db462a"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"3652275dafcc4f938340b3545a422536","deepnote_cell_type":"text-cell-p"},"source":"After training the model, it’s time to evaluate its performance. In this step, we will use the evaluation data (X_test) to make predictions and then calculate the accuracy of the model using the accuracy_score function. Finally, we will display the model's accuracy.","block_group":"7c4ebcd339834741ad62d359dd0bded7"},{"cell_type":"code","metadata":{"source_hash":"179a45a0","execution_start":1742857371032,"execution_millis":1,"execution_context_id":"fb64d43e-c606-4fed-a54f-587101cb25e2","cell_id":"34f5605ac6a54ff08b370b3833388c4f","deepnote_cell_type":"code"},"source":"# Make predictions on the test set\ny_pred = logistic_model.predict(X_test)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy}\")","block_group":"7705e90dd5dd4759922a980b1f32244a","execution_count":21,"outputs":[{"name":"stdout","text":"Accuracy: 0.6379183400267737\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/47743fac-9cc3-4607-bf3d-a9dcf8710879","content_dependencies":null},{"cell_type":"markdown","metadata":{"cell_id":"4401525c2cdf4124ac39160d4367f6bf","deepnote_cell_type":"separator"},"source":"<hr>","block_group":"cdc70f51143f401f9962771ca9a115fc"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"9525729a03c140009c90717dde59513d","deepnote_cell_type":"text-cell-h2"},"source":"## Create a Function to Predict Recommendation Status\r","block_group":"dfb1730144bb47deb83d9014d2ba19ca"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"2f29fdb971514444aa5a7365a99192fb","deepnote_cell_type":"text-cell-p"},"source":"In this step, we will create a function called predict_recommendation to predict the recommendation status of a new review.","block_group":"ba3c155f63ce4cd8939fc2ce44a8d6c2"},{"cell_type":"code","metadata":{"source_hash":"741b51e0","execution_start":1742857371112,"execution_millis":0,"execution_context_id":"fb64d43e-c606-4fed-a54f-587101cb25e2","cell_id":"d5bda59ee82a48aeb49dced969c8a8cc","deepnote_cell_type":"code"},"source":"def predict_recommendation(comment):\n    preprocessed_comment = preprocess_text(comment)\n    sentence_vector_comment = sentence_vector(preprocessed_comment)\n    X_comment = np.array([sentence_vector_comment])\n    prediction = logistic_model.predict(X_comment)\n    if prediction[0] == 2:\n        return \"no_idea\"\n    elif prediction[0] == 1:\n        return \"recommended\"\n    else:\n        return \"not_recommended\"","block_group":"1df7d810f4eb48fca21a9c5d53616337","execution_count":22,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"eb8687db","execution_start":1742857371172,"execution_millis":1,"execution_context_id":"fb64d43e-c606-4fed-a54f-587101cb25e2","cell_id":"ca2b27e8c2f94455af5faf603bb24125","deepnote_cell_type":"code"},"source":"new_comment = 'نخرید'\npredict_recommendation(new_comment)","block_group":"bd1cb09008854571b0cb04848917b8b5","execution_count":23,"outputs":[{"output_type":"execute_result","execution_count":23,"data":{"text/plain":"'not_recommended'"},"metadata":{}}],"outputs_reference":"dbtable:cell_outputs/56ac5252-5298-43ee-bac0-de218b8dfc31","content_dependencies":null},{"cell_type":"markdown","metadata":{"cell_id":"db28d708d5ba453a80ac3fce3bc5a582","deepnote_cell_type":"separator"},"source":"<hr>","block_group":"90ba46f8be90418f89b7bf1165f762ac"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"f0dd7d5a5f594cf0b13a9715b5929274","deepnote_cell_type":"text-cell-h2"},"source":"## Using machine learning for prediction","block_group":"86735c23fa7b4263b283091e6c3cd1a4"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"3793df63f8f041fe89e4a8a253658e07","deepnote_cell_type":"text-cell-p"},"source":"Now that the model has completed its learning process using the dataset and achieved satisfactory accuracy, we should use it to predict sentiment in comments. The comments are in the file comments.csv, and by applying the prediction function to them, we will generate two output files:","block_group":"dadf0b7700eb49369895d73d50b09d7f"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"2730a862401d4659b6ac6fb924c1ee10","deepnote_cell_type":"text-cell-p"},"source":"1- A file containing the comments along with the model's prediction for each comment.","block_group":"204a5f94d2ae4d65915527087919fd90"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"52ddbc5d41c8419ebbb2d8b91e5f0520","deepnote_cell_type":"text-cell-p"},"source":"2- A summary file of the prediction process including:","block_group":"d43428d6b7ff490691dc9a034ff23943"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"fa281619be2545838635e2ada4b6202a","deepnote_cell_type":"text-cell-p"},"source":"    - Total number of comments processed","block_group":"8f2a6072b52b4ff8b051c757b989185b"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"fe092e0d93f64727a79485e260e2aa34","deepnote_cell_type":"text-cell-p"},"source":"    - Number of \"recommended\" comments","block_group":"20679e0ab8674f86b18e78b8ccd1dc39"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"f3d2a75840f040a6bcc4d1e8e0ba9e47","deepnote_cell_type":"text-cell-p"},"source":"    - Number of \"not recommended\" comments","block_group":"8da2f132b1d7444f9a1d1454cd81e4c3"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"8358f901e08447ef85ebda53c14d7948","deepnote_cell_type":"text-cell-p"},"source":"    - Number of \"no idea\" comments","block_group":"d6e638453cfe4429b69377f4d6ed465b"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"95284aad7fd94f8c96f992ed942fd7bb","deepnote_cell_type":"text-cell-p"},"source":"    - Number of error cases","block_group":"0920482184e44b2fb6a990799ba05666"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"773053580708499186a4a5c7c0824ca3","deepnote_cell_type":"text-cell-p"},"source":"    - The model's accuracy","block_group":"db6ea067531c4234b3cce09c62080517"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"1423db44716847cc9478a42346d5ac98","deepnote_cell_type":"text-cell-p"},"source":"Both files will be saved as output.","block_group":"135c2cbf62204db09d06cb05486db949"},{"cell_type":"code","metadata":{"source_hash":"336aef07","execution_start":1742857371222,"execution_millis":3469,"execution_context_id":"fb64d43e-c606-4fed-a54f-587101cb25e2","cell_id":"d036e5019c5f4b3584ea9a3bde1e2e00","deepnote_cell_type":"code"},"source":"def predict_sentiments_for_file(input_file, output_file, summary_file, model_accuracy=None):\n    try:\n        comments_df = pd.read_csv(input_file, header=None, names=['comment'])\n    except Exception as e:\n        print(f\"Error reading input file: {e}\")\n        return\n    \n    results = []\n    error_count = 0\n    \n    for comment in tqdm(comments_df['comment'], desc=\"Predicting sentiments\"):\n        try:\n            sentiment = predict_recommendation(comment)\n            results.append({'comment': comment, 'sentiment': sentiment})\n        except Exception as e:\n            print(f\"Error predicting sentiment for '{comment}'. : {e}\")\n            results.append({'comment': comment, 'sentiment': 'error'})\n            error_count += 1\n    \n    results_df = pd.DataFrame(results)\n    \n    sentiment_counts = results_df['sentiment'].value_counts()\n    total_comments = len(results_df)\n    \n    summary_data = {\n        'Sentiment': [\n            'number of comments',\n            'recommended',\n            'not recommended', \n            'no idea',\n            'number of errors',\n            'model accuracy (%)'\n        ],\n        'Number': [\n            total_comments,\n            sentiment_counts.get('recommended', 0),\n            sentiment_counts.get('not_recommended', 0),\n            sentiment_counts.get('no_idea', 0),\n            error_count,\n            '-' \n        ],\n        'Percentage': [\n            100,\n            round(sentiment_counts.get('recommended', 0) / total_comments * 100, 2),\n            round(sentiment_counts.get('not_recommended', 0) / total_comments * 100, 2),\n            round(sentiment_counts.get('no_idea', 0) / total_comments * 100, 2),\n            round(error_count / total_comments * 100, 2),\n            round(model_accuracy * 100, 2) if model_accuracy is not None else '-'\n        ]\n    }\n    \n    summary_df = pd.DataFrame(summary_data)\n    \n    try:\n        results_df.to_csv(output_file, index=False, encoding='utf-8-sig')\n        print(f\"results saved in '{output_file}' .\")\n        \n        summary_df.to_csv(summary_file, index=False, encoding='utf-8-sig')\n        print(f\"result summary saved in '{summary_file}'.\")\n        \n        print(\"\\nresults summary:\")\n        print(summary_df.to_string(index=False))\n        \n    except Exception as e:\n        print(f\"Error saving results: {e}\")\n\ninput_csv = 'comments.csv'\noutput_csv = 'sentiment_results.csv'\nsummary_csv = 'sentiment_summary.csv'\n\n\npredict_sentiments_for_file(input_csv, output_csv, summary_csv, model_accuracy=accuracy)","block_group":"82257738f2804a278884befaa0168691","execution_count":24,"outputs":[{"name":"stderr","text":"Predicting sentiments: 100%|██████████| 3261/3261 [00:02<00:00, 1177.10it/s]\nresults saved in 'sentiment_results.csv' .\nresult summary saved in 'sentiment_summary.csv'.\n\nresults summary:\n         Sentiment Number  Percentage\nnumber of comments   3261      100.00\n       recommended   1747       53.57\n   not recommended    720       22.08\n           no idea    794       24.35\n  number of errors      0        0.00\nmodel accuracy (%)      -       63.79\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/edbd91d7-77c9-4bc4-996c-a0217e5b4bf2","content_dependencies":null},{"cell_type":"markdown","metadata":{"cell_id":"4ad3b2fa103f4ceea70ec4e547f7a396","deepnote_cell_type":"separator"},"source":"<hr>","block_group":"f4d13ae576da4375b15a31275b4dfbff"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"3fb9693ad0e34a1c9b092d5c37478eda","deepnote_cell_type":"text-cell-h3"},"source":"### Github.com : RezaGooner","block_group":"d20880de333a4d938dc4b1f20820d0d2"},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=0739d037-8289-409c-a01d-ddab9865ba9f' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote_notebook_id":"b00b9a43113d46b99a725ea4e307a15e"}}